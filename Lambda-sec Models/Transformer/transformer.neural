network TransformerModel {
  input: (None, 128, 512)  # Sequence length, embedding dim
  layers:
    TransformerEncoder(num_heads=8, ff_dim=2048, dropout=0.1) * 6
    GlobalAveragePooling1D()
    Dense(units=256, activation="relu")
    Output(units=10, activation="softmax");
  
  loss: "categorical_crossentropy"
  optimizer: Adam(learning_rate=0.0001)
  
  train {
    epochs: 20
    batch_size: 64
    validation_split: 0.2
  }
}